\documentclass[12pt,a4paper,english,onecolumn]{IEEEtran}

\usepackage{datetime}
\usepackage{caption}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{minted}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[paper=a4paper, top=2cm, bottom=3cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage[
    backend=bibtex,
    style=numeric,
    bibencoding=ascii,
    sorting=ynt
]{biblatex}
\addbibresource{bibliography}

\renewcommand*\contentsname{Table of Content}
\renewcommand*\listfigurename{List of Images}
\renewcommand\listingscaption{Example}
\renewcommand{\listtablename}{List of Tables}
\captionsetup[table]{name=Table}
\renewcommand*\figurename{Image}
\usemintedstyle{bw}
\hyphenpenalty=100000
\nocite{*}
\usepackage[T1]{fontenc}

\begin{document}

\title{Cyber Reasoning System}

\author{Claudiu-Florentin GHENEA, George-Andrei IOSIF\\
\emph{Faculty of Automatic Control and Computers \\ University POLITEHNICA of Bucharest \\ Splaiul Independen»õei 313, Bucharest, Romania, 060042}\\
\{claudiu.ghenea,george\_andrei.iosif\}@stud.acs.upb.ro}


\maketitle

\section{Project Idea}

A \textbf{cyber reasoning system} is an automated system for managing vulnerabilities inside executable programs, starting from discovery and continuing with exploiting, signature creation and self-healing.

The first viable prototypes were shown at a competition organized by DARPA back in 2016, Cyber Grand Challenge. Then, multiple solutions were installed on physical systems that were able to communicate with each, with a single purpose: to automatically attack other systems and to defend from them, earning points in a Capture The Flag competition.

From the top 3 finalists, that are still considered \textbf{state of the art} at the moment, Mayhem (the winner) became a commercial solution \footnote{https://forallsecure.com/mayhem-for-code} and Shellphish was published on GitHub as an open source project \footnote{https://github.com/shellphish} and became unmaintained. Since then, some others commercial software appeared, but the open source landscape remained the same.

In our thesis, we propose the creation of an \textbf{open source cyber reasoning system} using the latest advances in the binary analysis field. As the variety regarding the executables is high, we will niche our solution for ELF CLI executables, built from a C codebase on an x86 architecture.

\section{Work This Semester}

\subsection{Arguments Fuzzing with AFL++}

As mentioned in the previous report, the research regarding arguments fuzzing was slowed down due to technological choice. We managed to overcome this issue by integrating AFL++ into our cyber reasoning system. At the end of last semester, it executed fuzzing only over files and standard input.

We managed to detect the arguments with a custom implementation based on QBDI, a dynamic binary instrumentation engine. The remaining part was the generation of random inputs to be passed directly as arguments, in the vulnerability discovery module. We describe the solution in the last report.

To accomplish this task, the same \textbf{Docker image} from files and standard input fuzzing was used. The main difference is that the fuzzed binary is not the target binary anymore, but a \textbf{custom adapter} built in C. It takes the random input generated by AFL++ from \mintinline{text}{stdin}. Then, the input is tokenized by using space (\mintinline{text}{0x20} ASCII) as a delimiter, and the resulted \mintinline{text}{char *} sequence is used as \mintinline{text}{argv} for an \mintinline{text}{execve} syscall.

The initial image of the process, as it is created by the fuzzer, is the adapter's image. By using the described approach, the syscall replaces the process image with the real target binary's one, having as arguments a random string of characters. An arguments' mismanagement can cause a crash. AFL++ will be able to detect and report it.

\subsection{Vulnerability Analytics Module}

\subsubsection{Introduction}

During our research in the previous semester, we encountered challenges in finding a Dynamic Taint Analysis (DTA) tool that met our specific requirements. Despite evaluating multiple solutions, we found that none of them were suitable for our needs. The main reason for this was that the majority of DTA tools we came across relied on a predefined set of rules for determining taint propagation from sources to sinks during program execution. This rule-based approach requires accurate rules to be specified for each operation that needs to be monitored, which makes it difficult to define general rules that would cover all cases of interest.
The last tool that we have tested from this field was TaintGrind \cite{tainggrind}, this tool required modification over the source code of the binary, source code which is not available to us as our research focuses on blackbox analysis and testing. To conclude, we were unable to find a suitable DTA tool that provided the flexibility and versatility that we needed for our research.

With this in mind, we redirected our focus towards tools that are capable of performing crash exploration and triaging. These tools provide us with a deeper understanding of what occurs when a binary crashes under a specific input generated by our fuzzing module. By utilizing these tools, we can gain valuable insight into the behavior of binary programs, enabling us to identify potential security vulnerabilities, better understand their underlying causes and use this information as input for our next modules. For this,  we evaluated (as mentioned in the last semester report) Angr/Rex \cite{angr} as a state-of-the-art crash exploration and triaging tool that we can integrate in our CRS. This tool and our implementation will be presented in the following section.

\subsubsection{Angr/Rex}

Angr/Rex \cite{angr} is a leading-edge tool that provides crash triaging and crash exploration capabilities with support for ELF binaries. Developed by Mechanical Phish \cite{mecphish} as part of DARPA's CGC, Rex leverages the capabilities of Angr to support automated exploitation while also offering crash triaging and crash exploration capabilities. It has the ability to generate a crash by providing a binary file and a crashing input.

Rex leverages various concepts and tools to achieve its crash triaging and exploration features. It utilizes the concepts of \textbf{targets} and \textbf{analyzers}, inherited from Angr/Archr \cite{archr}, as follows:

\begin{itemize}
    \item The target concept refers to the environment in which the provided binary will be executed. This includes a Docker Image configuration, which generates a container for the execution and analysis of the binary, or a Local Target, which refers to the user's actual machine. It is recommended to use Docker Images as they offer more versatility and faster responses, as access within the resulting container is unrestricted.
    \item The concept of analyzers refers to the type of analysis to be performed within the specified target. Examples of such analysis include tracing the target, extracting only the core dump, and launching the target with gdb.
\end{itemize}

\subsubsection{Implementation}

As previously presented, we have selected Rex as our tool for crash triaging and exploration. In an effort to achieve optimal isolation and deployment, for our implementation, we have created a custom Docker image to serve as its environment.

This image is deployed as a special container, it is called special because it requires access to the host Docker socket (the Docker engine) in order to be able to build images and create new containers (as we have chosen Docker Image configuration to be the \textbf{target} for our analysis).

The entire crash triaging and exploration process is \textbf{orchestrated through Python} code to streamline the analysis of target binaries. This includes automatically creating custom tailored Docker images for the selected binary, starting Rex to instrument the newly created container, providing required input data and monitor the binary for any crashes. In the event of a crash, a crash type will be assigned, and relevant crash information will be provided in a CSV format, information such as the basic block address, the function where the crash occurred and register values.

For the presented implementation, we have considered only a subset of the possible input streams a binary could receive. These include:

\begin{itemize}
    \item Standard input: Those inputs are provided after the binary start executing using mechanisms provided by Rex.
    \item Arguments specified at startup: Those arguments are specified in the \mintinline{text}{ENTRYPOINT} command of the custom Docker image.
\end{itemize}

\subsection{Automatic Exploit Generation}

\subsubsection{Introduction}

The automatic exploit generation module (abbreviated AEG) will be directly linked to other two modules in order to receive its required data. Firstly, the vulnerability detection module should detect a vulnerability and generate a proof of vulnerability (abbreviated PoV). The information about the PoV will be enriched by the vulnerability analytics module, adding details about context and exploitability.

To determine the current academic progress in the automatic exploit generation field, we researched multiple papers related to this subject. All can be split into the following categories:

\begin{itemize}
    \item Papers describing an AEG (sub)system \cite{aig} \cite{mayhem} \cite{bop} \cite{bof_aeg}: The majority of the studied papers approach the problem by coupling it with the vulnerability detection one. In such way, the systems detect \textbf{weaknesses} in programs by leveraging diverse techniques and creating functional \textbf{exploits}.
    \item Papers summarizing the current AEG context \cite{crs_aeg_survey} \cite{alphahacking} \cite{code_reuse_survey}: This category does not present specific implementation of an AEG system, but review techniques and approaches that were proposed in functional cyber reasoning systems (for example, those from DARPA's Cyber Grand Challenge) and academia, as standalone exploitation research.
\end{itemize}

A summary of the most common weaknesses, mitigations, (support) exploitation techniques, and outcomes detailed or used in the studied papers are listed in the below lists:

\begin{itemize}
    \item \textbf{Weaknesses} \begin{itemize}
        \item \textbf{Out-of-bound memory writes}: Consists of writing data whose length exceeds the buffer length. There are two types of allocation: on \textbf{stack}, for variables defined in function, or \textbf{heap}, when the buffer is dynamically allocated. If the stack is targeted, then the \textbf{return address} and \textbf{old base pointer} can be overwritten.
        \item \textbf{Type overflow}: The correct result of an operation could not be stored in a certain type of variables.
        \item \textbf{Tainted format strings}: The first parameter of functions working with a format string (such as \mintinline{text}{printf}) is user controlled.
        \item \textbf{Information leaks}: The program divulges information about its execution (for example, memory layout).
    \end{itemize}
    \item \textbf{Mitigations} \begin{itemize}
        \item \textbf{Address Space Layout Randomization} (abbreviated ASLR): Segments such as the stack, dynamic libraries, and heap are placed randomly in the process memory space.
        \item \textbf{Position Independent Execution}: The code segment is placed, as in ASLR, randomly in the memory space. The program should be compiled with Position Independent Code enabled.
        \item \textbf{NX bit}: If enabled, the bit prevents a page to be executable and writable in the same time.
        \item \textbf{Stack canaries}: The canaries are values placed on the stack after entering a frame. It is checked on return and, if the check fails, then the execution is aborted.
        \item \textbf{Control flow integrity}: Permits only certain code, part of the valid control flow graph, to be executed. New code, injected by attackers in process memory, could not be executed anymore.
    \end{itemize}
    \item \textbf{Exploitation techniques (including support ones)} \begin{itemize}
        \item \textbf{Shellcode}: Machine code is injected in process memory and executed by chaining with other technique for hijacking the execution flow.
        \item \textbf{Return-oriented programming}: Unlike shellcodes, which inject new code, ROPs reuse code already existent in process memory. A chain of gadgets is created and executed by coupling the technique with execution flow hijack.
        \item \textbf{Data-oriented programming}: The accent in DOP resides on data flow and how it can be altered by using existent code constructs (similar to ROP).
        \item \textbf{NOP sleds}: This support technique is used with shellcodes to increase the probability that it will be executed correctly. The shellcode is prefixed with a \mintinline{text}{nop} sequence such that the execution could be redirected (for example, with return address overwrites) anywhere on the sled. This is in contrast to raw shellcode, when the execution needs to start from the first shellcode byte.
    \end{itemize}
    \item \textbf{Outcomes} \begin{itemize}
        \item \textbf{Code execution}: The programs execute the code desired by the attacker.
        \item \textbf{Denial of service}: The program functioning is blocked or halted at all.
        \item \textbf{Information leaks}: Sensitive information is extracted.
    \end{itemize}
\end{itemize}

Regarding the full data flow and functioning of AEG systems, \cite{archr} can be seen as an example. It has an approach similar to what we have chosen for our cyber reasoning system. Firstly, the proposed system gets basic information about activated mitigations, such as NX bits and ASLR. After vulnerabilities are detected by their mining module, the protections are bypassed with returns to system libraries, in the case of NX bits, and \mintinline{text}{jmp esp} gadgets, as in ASLR bypass. Then an SMT solver is used to automatically build an exploit considering the constraints. The validation of the whole process was made with binaries from the Cyber Grand Challenge.

\subsubsection{Implementation}

As seen in the previous subsection, the landscape of the automatic exploit generation is vast. For OpenCRS, we selected a bunch of weaknesses and exploits to demonstrate the impact an automated exploitation can have:

\begin{itemize}
    \item \textbf{Mitigations}: NX bit and ASLR;
    \item \textbf{Weaknesses}: instruction pointer overwrite (leveraging a stack buffer overflow) and format string attacks;
    \item \textbf{Exploit techniques}, all with code execution as desired \textbf{outcome}: ret2win, shellcodes, and return-oriented programming;
\end{itemize}

The mitigation detection was implemented with the help of \textbf{pwntools}, a Python 3 binary exploitation package. In this manner, the automatic exploit generation module is able to detect usage of ASLR, NX bit, stack canaries, relocation read-only, Position Independent Code, FORTIFY-SOURCE, and Address Sanitizer.

Besides the mitigations, the target executable is further inspected by checking if \textbf{relevant characteristics} are present:

\begin{itemize}
    \item Segments that can be written, read, and executed: Can be leveraged during attacks with shellcodes, against buffer overflows. A shellcode can be put directly on the segment and the return address is overwritten with the address of the segment's beginning.
    \item An executable stack: Same as above, but the segment is in fact the whole stack, which contains a writable buffer.
    \item Sensitive functions: Functions with names containing strings such as "\textit{secret}", "\textit{shell}", and "\textit{system}", can be called as some functionality relevant to an attacker can be unlocked.
\end{itemize}

The single weakness tackled this semester is \textbf{instruction pointer overwrite}. After the vulnerability detection manages to overwrite the instruction pointer, the automatic exploit generation will firstly fingerprint the weakness by executing the following sequence of steps:

\begin{enumerate}
    \item Generate a random De Bruijn sequence (noted \mintinline{text}{<seq>}).
    \item  Run the executable with the generated sequences.
    \item  Get the value of the instruction pointer when the crash occurred (noted \mintinline{text}{<eip_subseq>}).
    \item  Deduce the offset between the buffer's start and the instruction pointer by finding \mintinline{text}{<eip_subseq>} in \mintinline{text}{<seq>}.
\end{enumerate}

In the current module version, the weakness is then exploited with \textbf{ret2win}. The program is run repeatedly with the addresses of the sensitive functions. The behavior should be then watched to detect non-secure actions: shell spawning, produced outputs, etc.

\subsection{Binary Patching}

Another topics that we tackled this semester are that of Binary rewriting and Binary lifting. As we have just barely scratched the surface of those topics. In the following section, we will briefly go through explaining those therms and present some identified tools that we will be further investigating.

\subsubsection{Introduction to Binary Rewriting}

\textbf{Binary rewriting} \cite{hack_to_elaborate} refers to the process of modifying an existing binary without having access to the source code. This technique is used in a variety of applications, ranging from performance optimization to vulnerability mitigation and code obfuscation.

Binary rewriting can be performed at different levels of abstraction, ranging from low-level machine code to high-level source code. The level of abstraction at which binary rewriting is performed can have a significant impact on the complexity of the rewriting process, as well as on the functionality and performance of the resulting binary.

Binary rewriting can be either static (performed on the binary file) or dynamic (performed during the program execution) and regardless of it's type it can be split into four steps, as follows (as presented in \cite{hack_to_elaborate}):

\begin{itemize}
    \item Parsing: This step consists in obtaining the raw instruction stream from the executable, find address ranges, data section and pass all the information to the disassembler. This step is equivalent to converting the binary code back into assembly language (or another language, as we will see in the following section).
    \item Analysis: This step recovers the structure of the program, groups the identified functions, marks variables with the appropriate data type and generates a \textbf{control flow graph} (CFG).
    \item Transformation: In this step alterations to the binary can be done, this involves inserting changes at specific locations known as \textbf{instrumentation points}. These points can either change the control flow or add new instructions.
    \item Code Generation: The final step is to integrate the changes and reassemble the modified code back into a binary format so that it can still be executed. This requires ensuring that the code can be properly loaded and executed by the target system.
\end{itemize}

\subsubsection{Introduction to Binary Lifting}

The term \textbf{binary lifting} \cite{sok} refers to the conversion of an executable into an intermediate representation (IR) of a compiler framework, in this case, the widely used LLVM \cite{llvm}. The intermediate representation is a data structure or format utilized by compilers to internally represent source code. This study \cite{sok} evaluates various lifting tools by subjecting them to real-world scenarios such as \textbf{decompilation}, \textbf{accuracy}, and \textbf{distinguishability}. Our focus was solely on \textbf{decompilation} capabilities and \textbf{correctness}, seeking to determine the best tool that can perform an effective decompilation of a binary while preserving its correct functionality upon recompilation.

Using the aforementioned paper, we have identified a selection of open-source, state-of-the-art binary Lifters that could potentially be beneficial for our implementation of the binary patching component in the cyber reasoning system architecture. These tools include Mcsema \cite{mcsema}, RetDec \cite{retdec}, Mctoll \cite{llvm_mctoll}, and BinRec \cite{binrec}. Tools that we plan to further investigate.

\subsection{Complementary Tasks for All Repositories}

Besides the researching and implementation work, we completed complementary tasks that are not functionally required, but ease the development and use. They consisted mainly in ensuring, for all repositories:

\begin{enumerate}
    \item \textbf{Updated \mintinline{text}{README.md}}, with sections such as description, internal functioning, and setup and usage guides;
    \item  \textbf{Adjusted \mintinline{text}{pyproject.toml}} files, that are specific to Poetry (a Python dependency manager) and which describes the Python package; and
    \item  \textbf{Consistent} shape of the \textbf{CLI interfaces} present in each module.
\end{enumerate}

\section{Conclusion. Planned Work}

As the next semester is our last in this master program, we plan to bring the cyber reasoning system to \textbf{a functional state}. The summer version will integrate all developed modules and will orchestrate them to achieve its ultimate goal: finding, exploiting, and patching vulnerabilities in binary programs.

Thus, the functionality of the \textbf{exploitation module} will be expanded even more, based on the previous research. It will be able to fingerprint weaknesses related to format strings. In addition, shellcodes and return oriented programming will be added as exploitation techniques, while counteroffensives against NX bit and ASLR will be developed too. In this manner, the CRS will be able to leverage multiple attack paths to achieve one outcome desired by an attacker (for example, code execution).

In regard to the \textbf{binary patching module}, we have conducted research on the concepts of binary rewriting and binary lifting, and identified potential tools that require testing and experimentation in order to successfully develop a functioning binary patching module. Our final goal here is to develop a module that can integrate the proof of vulnerability, the comprehensive report generated by the crash exploration module, and the binary itself, to produce a new and hardened binary that is protected against the identified weaknesses.

With these two modules finished, we will be capable of \textbf{integrating} all modules in a single monolith representing the cyber reasoning system. We expect that the coupling will be mostly precise, as one Python module will call functions from another Python module. There may be some issues due to inconsistencies in required parameters, but the changes will be performed punctually.

After the integration will be completed, we will execute \textbf{manual tests} with toy executables, and \textbf{automatic} one with executables from our dataset module. The last mentioned will be filtered such that the only attacked binaries are those vulnerable to specific, supported weaknesses (for example, buffer overflows that may overwrite the instruction pointer).

Lastly, we want to wrap up our thesis by ensuring a high quality of the open source codebase such that further contributors will have a proper deep-dive. This will solve minor tasks that were postponed, but repetitive for all repositories:

\begin{itemize}
    \item Centralizing all configuration aspects into a file;
    \item Logging information regarding the operations executed by each module;
    \item Linting, including ensuring and checking types;
    \item Auto-formatting; and
    \item Generating a project documentation, eventually with a standalone web wiki.
\end{itemize}

\section{Bibliography}
\nocite{*}
\printbibliography[heading=none]

\end{document}